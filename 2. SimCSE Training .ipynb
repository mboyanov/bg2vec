{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45155cf8-b83a-47f0-b7ac-378cf02c3688",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "import os\n",
    "import sys\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from accelerate import Accelerator, DistributedDataParallelKwargs\n",
    "from accelerate.logging import get_logger\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    MODEL_FOR_MASKED_LM_MAPPING,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    TrainerCallback,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.trainer_utils import seed_worker\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "from llm2vec import LLM2Vec\n",
    "from llm2vec.dataset.utils import load_dataset\n",
    "from llm2vec.loss.utils import load_loss\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "MODEL_CONFIG_CLASSES = list(MODEL_FOR_MASKED_LM_MAPPING.keys())\n",
    "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb920067-2746-4677-8561-aee301f30078",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bg2vec.arguments import simcse_parser\n",
    "    \n",
    "model_args, data_args, training_args, custom_args = simcse_parser.parse_json_file(\n",
    "        \"model_configurations/bggpt-7b.json\"\n",
    "    )\n",
    "if training_args.ddp_find_unused_parameters:\n",
    "    kwargs = [\n",
    "        DistributedDataParallelKwargs(\n",
    "            dim=0,\n",
    "            broadcast_buffers=True,\n",
    "            bucket_cap_mb=25,\n",
    "            find_unused_parameters=True,\n",
    "            check_reduction=False,\n",
    "            gradient_as_bucket_view=False,\n",
    "        )\n",
    "    ]\n",
    "else:\n",
    "    kwargs = []\n",
    "accelerator = Accelerator(kwargs_handlers=kwargs)\n",
    "\n",
    "set_seed(training_args.seed)\n",
    "\n",
    "if training_args.gradient_checkpointing:\n",
    "    training_args.gradient_checkpointing_kwargs = {\"use_reentrant\": False}\n",
    "\n",
    "train_dataset = load_dataset(\n",
    "    data_args.dataset_name,\n",
    "    split=\"train\",\n",
    "    file_path=data_args.dataset_file_path,\n",
    ")\n",
    "\n",
    "train_examples = [\n",
    "    train_dataset[i]\n",
    "    for i in tqdm(\n",
    "        range(len(train_dataset)),\n",
    "        desc=\"Loading train examples...\",\n",
    "        disable=not accelerator.is_main_process,\n",
    "    )\n",
    "]\n",
    "\n",
    "torch_dtype = (\n",
    "    model_args.torch_dtype\n",
    "    if model_args.torch_dtype in [\"auto\", None]\n",
    "    else getattr(torch, model_args.torch_dtype)\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28aa5906-c949-4d04-9df1-0a32ffead460",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LLM2Vec.from_pretrained(\n",
    "    base_model_name_or_path=model_args.model_name_or_path,\n",
    "    enable_bidirectional=model_args.bidirectional,\n",
    "    peft_model_name_or_path=model_args.peft_model_name_or_path,\n",
    "    merge_peft=True,\n",
    "    pooling_mode=model_args.pooling_mode,\n",
    "    max_length=model_args.max_seq_length,\n",
    "    torch_dtype=torch_dtype,\n",
    "    attn_implementation=model_args.attn_implementation,\n",
    "    attention_dropout=custom_args.simcse_dropout,\n",
    ")\n",
    "\n",
    "# model organization is LLM2VecModel.model -> HF Model, we have to apply PEFT to the inner model\n",
    "model.model = initialize_peft(\n",
    "    model.model,\n",
    "    lora_r=custom_args.lora_r,\n",
    "    lora_alpha=2 * custom_args.lora_r,\n",
    "    lora_dropout=custom_args.lora_dropout,\n",
    ")\n",
    "\n",
    "tokenizer = model.tokenizer\n",
    "\n",
    "train_loss = load_loss(custom_args.loss_class, scale=custom_args.loss_scale)\n",
    "\n",
    "data_collator = DefaultCollator(model)\n",
    "\n",
    "trainer = SimCSETrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_examples,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    loss_function=train_loss,\n",
    ")\n",
    "\n",
    "if custom_args.stop_after_n_steps is not None:\n",
    "    trainer.add_callback(StopTrainingCallback(custom_args.stop_after_n_steps))\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
