{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d09689f6-e9ba-482a-a3e6-31c2b05f696a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "037f7711-7760-422e-be4a-350fabf4d6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "841d5810-bc64-497c-99c8-15400e698f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bg2vec.arguments import *\n",
    "from bg2vec.data_util import *\n",
    "from bg2vec.model import *\n",
    "from bg2vec.training import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de8866d5-bda1-43de-962f-521a20a598e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "#os.putenv(\"HF_TOKEN\", \"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a184357-2ecf-44c2-b230-3147a2ca9cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args, data_args, training_args, custom_args = parser.parse_json_file(\"model_configurations/bggpt-7b.json\")\n",
    "\n",
    "config_kwargs = {\n",
    "    \"cache_dir\": model_args.cache_dir,\n",
    "    \"revision\": model_args.model_revision,\n",
    "    \"token\": model_args.token,\n",
    "    \"trust_remote_code\": model_args.trust_remote_code,\n",
    "}\n",
    "if training_args.gradient_checkpointing:\n",
    "    training_args.gradient_checkpointing_kwargs = {\"use_reentrant\": False}\n",
    "# Set seed before initializing model.\n",
    "\n",
    "transformers.set_seed(training_args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347721df-c9f1-45df-b46b-636a0287b5f3",
   "metadata": {},
   "source": [
    "## Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "904457cc-f6a3-4055-8601-2136a4768642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "llm2vec.models.bidirectional_mistral.MistralBiForMNTP"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoConfig)\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_args.model_name_or_path, **config_kwargs\n",
    ")\n",
    "model_class = get_model_class(config)\n",
    "model_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efe56443-e980-4a45-94f6-907fe925e49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "797eea78b1a7483ba85491571855ca09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch_dtype = (\n",
    "    model_args.torch_dtype\n",
    "    if model_args.torch_dtype in [\"auto\", None]\n",
    "    else getattr(torch, model_args.torch_dtype)\n",
    ")\n",
    "model = model_class.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
    "    config=config,\n",
    "    cache_dir=model_args.cache_dir,\n",
    "    revision=model_args.model_revision,\n",
    "    token=model_args.token,\n",
    "    trust_remote_code=model_args.trust_remote_code,\n",
    "    torch_dtype=torch_dtype,\n",
    "    low_cpu_mem_usage=model_args.low_cpu_mem_usage,\n",
    "    attn_implementation=model_args.attn_implementation,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "546f26f2-5b3f-4f65-8ddc-d081e8c4e7ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MistralBiForMNTP(\n",
       "  (model): MistralBiModel(\n",
       "    (embed_tokens): Embedding(38000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x ModifiedMistralDecoderLayer(\n",
       "        (self_attn): ModifiedMistralFlashAttention2(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm()\n",
       "        (post_attention_layernorm): MistralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=38000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9825f73e-479a-4083-aff6-4cac79118cb8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mself_attn\u001b[38;5;241m.\u001b[39mis_causal\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.model.layers[0].self_attn.is_causal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "58d418ef-f54e-4ad7-a68f-dd4012c4d27e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's Lora trainable parameters:\n",
      "trainable params: 41,943,040 || all params: 7,177,179,136 || trainable%: 0.5843944982453898\n"
     ]
    }
   ],
   "source": [
    "model = initialize_peft(\n",
    "    model,\n",
    "    lora_r=custom_args.lora_r,\n",
    "    lora_alpha=2 * custom_args.lora_r,\n",
    "    lora_dropout=custom_args.lora_dropout,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fd998ac9-1fee-4f12-8c43-079e2d8e3c2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModifiedMistralDecoderLayer(\n",
       "  (self_attn): ModifiedMistralFlashAttention2(\n",
       "    (q_proj): lora.Linear(\n",
       "      (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (lora_dropout): ModuleDict(\n",
       "        (default): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (lora_A): ModuleDict(\n",
       "        (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "      )\n",
       "      (lora_B): ModuleDict(\n",
       "        (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "      )\n",
       "      (lora_embedding_A): ParameterDict()\n",
       "      (lora_embedding_B): ParameterDict()\n",
       "    )\n",
       "    (k_proj): lora.Linear(\n",
       "      (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (lora_dropout): ModuleDict(\n",
       "        (default): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (lora_A): ModuleDict(\n",
       "        (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "      )\n",
       "      (lora_B): ModuleDict(\n",
       "        (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "      )\n",
       "      (lora_embedding_A): ParameterDict()\n",
       "      (lora_embedding_B): ParameterDict()\n",
       "    )\n",
       "    (v_proj): lora.Linear(\n",
       "      (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (lora_dropout): ModuleDict(\n",
       "        (default): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (lora_A): ModuleDict(\n",
       "        (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "      )\n",
       "      (lora_B): ModuleDict(\n",
       "        (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "      )\n",
       "      (lora_embedding_A): ParameterDict()\n",
       "      (lora_embedding_B): ParameterDict()\n",
       "    )\n",
       "    (o_proj): lora.Linear(\n",
       "      (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (lora_dropout): ModuleDict(\n",
       "        (default): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (lora_A): ModuleDict(\n",
       "        (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "      )\n",
       "      (lora_B): ModuleDict(\n",
       "        (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "      )\n",
       "      (lora_embedding_A): ParameterDict()\n",
       "      (lora_embedding_B): ParameterDict()\n",
       "    )\n",
       "    (rotary_emb): MistralRotaryEmbedding()\n",
       "  )\n",
       "  (mlp): MistralMLP(\n",
       "    (gate_proj): lora.Linear(\n",
       "      (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (lora_dropout): ModuleDict(\n",
       "        (default): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (lora_A): ModuleDict(\n",
       "        (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "      )\n",
       "      (lora_B): ModuleDict(\n",
       "        (default): Linear(in_features=16, out_features=14336, bias=False)\n",
       "      )\n",
       "      (lora_embedding_A): ParameterDict()\n",
       "      (lora_embedding_B): ParameterDict()\n",
       "    )\n",
       "    (up_proj): lora.Linear(\n",
       "      (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (lora_dropout): ModuleDict(\n",
       "        (default): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (lora_A): ModuleDict(\n",
       "        (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "      )\n",
       "      (lora_B): ModuleDict(\n",
       "        (default): Linear(in_features=16, out_features=14336, bias=False)\n",
       "      )\n",
       "      (lora_embedding_A): ParameterDict()\n",
       "      (lora_embedding_B): ParameterDict()\n",
       "    )\n",
       "    (down_proj): lora.Linear(\n",
       "      (base_layer): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (lora_dropout): ModuleDict(\n",
       "        (default): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (lora_A): ModuleDict(\n",
       "        (default): Linear(in_features=14336, out_features=16, bias=False)\n",
       "      )\n",
       "      (lora_B): ModuleDict(\n",
       "        (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "      )\n",
       "      (lora_embedding_A): ParameterDict()\n",
       "      (lora_embedding_B): ParameterDict()\n",
       "    )\n",
       "    (act_fn): SiLU()\n",
       "  )\n",
       "  (input_layernorm): MistralRMSNorm()\n",
       "  (post_attention_layernorm): MistralRMSNorm()\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.base_model.model.layers[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94bd869-c8b7-47b0-82bd-caa0f5440d82",
   "metadata": {},
   "source": [
    "## Setting up data collation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d4a20063-96a6-4d1a-bbc0-74cb2da406b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='INSAIT-Institute/BgGPT-7B-Instruct-v0.2', vocab_size=38000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer_kwargs = {\n",
    "    \"cache_dir\": model_args.cache_dir,\n",
    "    \"use_fast\": model_args.use_fast_tokenizer,\n",
    "    \"revision\": model_args.model_revision,\n",
    "    \"token\": model_args.token,\n",
    "    \"trust_remote_code\": model_args.trust_remote_code,\n",
    "}\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_args.model_name_or_path, **tokenizer_kwargs\n",
    ")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4449374e-ea19-4898-873b-f3329f148efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if tokenizer.mask_token is None:\n",
    "    if custom_args.mask_token_type == \"blank\":\n",
    "        tokenizer.mask_token = \"_\"\n",
    "    elif custom_args.mask_token_type == \"eos\":\n",
    "        tokenizer.mask_token = tokenizer.eos_token\n",
    "    elif custom_args.mask_token_type == \"mask\":\n",
    "        tokenizer.add_tokens([\"<mask>\"])\n",
    "        tokenizer.mask_token = \"<mask>\"\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"mask_token_type {custom_args.mask_token_type} is not supported.\"\n",
    "        )\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "537b7c09-e41e-44b3-a8b5-e88180cb1ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModelingWithFullMasking(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm_probability=data_args.mlm_probability\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "278f60c0-b741-4b98-888e-55ad2345d826",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2492a7f6-7da4-40f9-ae77-55af933e5a5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28730"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator.tokenizer.vocab['_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b57f9c5d-57cb-43aa-a152-b281eed492ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[[28730,     6,     9, 28730, 28730, 28730, 28730, 28730, 28730,     4]]]),\n",
       " 'labels': tensor([[[   4, -100, -100,    8,    0,    4,    7,    0,    5, -100]]])}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator( (torch.randint(0, 10, (1, 10)), ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bd7760-b836-45a0-9df0-51d5a79ee194",
   "metadata": {},
   "source": [
    "## Loading up the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6c3899-23aa-4053-85db-182ea59d36e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = datasets.load_from_disk(\"grouped_512\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c90ef94-5f74-4966-b2d2-753a1bfa11f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "if data_args.max_train_samples is not None:\n",
    "    max_train_samples = min(len(train_dataset), data_args.max_train_samples)\n",
    "    train_dataset = train_dataset.select(range(max_train_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfae6f5-9f43-4754-86d9-6e7d2c162f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = tokenized_datasets[\"validation\"]\n",
    "if data_args.max_eval_samples is not None:\n",
    "    max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)\n",
    "    eval_dataset = eval_dataset.select(range(max_eval_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc331217-d0f3-4234-9b34-aa7e37f1c099",
   "metadata": {},
   "source": [
    "## Setting up the trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "74c2b9ee-7ad1-43c5-a31a-1b623348b0a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m MNTPTrainer(\n\u001b[1;32m      2\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m      3\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m----> 4\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39m\u001b[43mtrain_dataset\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m training_args\u001b[38;5;241m.\u001b[39mdo_train \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m      5\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39meval_dataset \u001b[38;5;28;01mif\u001b[39;00m training_args\u001b[38;5;241m.\u001b[39mdo_eval \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m      6\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[1;32m      7\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_collator,\n\u001b[1;32m      8\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m training_args\u001b[38;5;241m.\u001b[39mdo_eval \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     11\u001b[0m     preprocess_logits_for_metrics\u001b[38;5;241m=\u001b[39mpreprocess_logits_for_metrics\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m training_args\u001b[38;5;241m.\u001b[39mdo_eval \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     17\u001b[0m trainer\u001b[38;5;241m.\u001b[39madd_callback(StopTrainingCallback(custom_args\u001b[38;5;241m.\u001b[39mstop_after_n_steps))\n\u001b[1;32m     18\u001b[0m trainer\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mremove_callback(transformers\u001b[38;5;241m.\u001b[39mintegrations\u001b[38;5;241m.\u001b[39mintegration_utils\u001b[38;5;241m.\u001b[39mWandbCallback)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "trainer = MNTPTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset if training_args.do_train else None,\n",
    "    eval_dataset=eval_dataset if training_args.do_eval else None,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    "    if training_args.do_eval and not is_torch_tpu_available()\n",
    "    else None,\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics\n",
    "    if training_args.do_eval and not is_torch_tpu_available()\n",
    "    else None,\n",
    "\n",
    ")\n",
    "\n",
    "trainer.add_callback(StopTrainingCallback(custom_args.stop_after_n_steps))\n",
    "trainer.callback_handler.remove_callback(transformers.integrations.integration_utils.WandbCallback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81118d3c-535d-4b36-a36c-99ca09cdb303",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cffe07c5-2652-4641-8226-bc2113ae687a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_result \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241m.\u001b[39mtrain()    \n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "train_result = trainer.train()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470be2a4-c975-4cb2-b925-685f5c46ecd5",
   "metadata": {},
   "source": [
    "## Model export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31d0534-def7-45b2-957b-e0fe4dd63555",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"bggpt-mntp-pretrained-2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f742637-a3df-46d0-8ec7-5ac9ddbaa9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()  # Saves the tokenizer too for easy upload\n",
    "metrics = train_result.metrics\n",
    "\n",
    "max_train_samples = (\n",
    "    data_args.max_train_samples\n",
    "    if data_args.max_train_samples is not None\n",
    "    else len(train_dataset)\n",
    ")\n",
    "metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n",
    "\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "trainer.save_state()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
